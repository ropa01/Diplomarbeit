
In this section some general considerations about mixed-integer non-linear programming (MINLP) will be presented.
First the most commonly applied solution techniques will briefly be discussed on the basis of a review on the
subject by Grossmann \cite{Grossmann.2002}. Then then an alternative approach to solve this class of problems,
based on a continuous reformulation of the discrete decision variables, which has recently been proposed
\cite{Kraemer.2010,Stein.2004} will also be introduced. This reformulation is also the basis for a comparison of different
approaches to solve this class of problem within the process simulation environment \gproms.

    \subsection{Solution techniques}

    As previously stated, the following discussion of general solution techniques for MINLP's is largely based
    in the comprehensive review by I. E. Grossmann \cite{Grossmann.2002}.

    Several approaches to tackle this type of problem have successfully been applied to a multitude of cases for
    several years now. In general four major types of solution algorithms can be distinguished.
    \begin{itemize}
        \item branch and bound
        \item outer approximation
        \item generalized benders
        \item extended cutting plane
        \item LP/NLP ...
    \end{itemize}
    All these algorithms make use of a limited number of common subproblems, which are then solved
    in different configurations. Therefore these subproblems can previously be discussed and will be referred
    to when going elaborating on different algorithms. Furthermore it needs to be emphasised, that the presented
    solution techniques are designed for convex problems, and only in those cases an optimal solution can
    be found with a degree of confidence. While they can be applied to the more general non-convex case, only
    local optimality can be assured.

    The general case of a MINLP takes the form
    \program{prog:stdMINLP}{
        & \minimize{x,y} & & C = f(x,y) \\
        & \st & & g_j(x,y) \leq 0, \; \; j \in \mathset{J} \\
        & & & x \in \mathset{X}, \; y \in \mathset{Y}
    }

    If the discrete variables in the discrete-continuous program are relaxed, the resulting NLP relaxation
    can be written as
    \program{prog:relaxedNLP}{
        & \minimize{x,y} & & C_{LB}^k = f(x,y) \\
        & \st & & g_j(x,y) \leq 0, \; \; j \in \mathset{J} \\
        & & & x \in \mathset{X}, \; \; y \in \mathset{Y}_R \\
        & & & y_i \leq \alpha_i^k, \; i \in \mathset{I}_{FL}^k \\
        & & & y_i \geq \beta_i^k, \; i \in \mathset{I}_{FU}^k
    }
    Here $\mathset{Y}_R$ denotes the relaxed set of the integer set $\mathset{Y}$, $\mathset{I}_{FL}^k$ and
    $\mathset{I}_{FU}^k$ are subsets of the indices denoting the entire set of integer variables.
    The relaxed integers contained in these sets are  bounded to the values $\alpha_i^k$ and $\beta_i^k$ respectively.
    These bounds are lower and upper bound taken from the previous iteration of the algorithm in question. If
    $\mathset{I}_{FL}^k = \emptyset$ and $\mathset{I}_{FU}^k = \emptyset$ are empty sets, problem \progref{prog:relaxedNLP}
    denotes the fully relaxed problem, initially solved in all algorithms. The optimal solution to this initially solved
    problem ($C_{LB}^0$) poses an absolute lower bound to \progref{prog:stdMINLP} since all variables are fully relaxed.

    If all discrete variables are fixed at a given value, the NLP subproblem for fixed $y^k$ results, as
    only continuous variables are being considered.
    \program{prog:fixedyNLP}{
        & \minimize{x} & & C_{LB}^k = f(x,y^k) \\
        & \st & & g_j(x,y^k) \leq 0, \; \; j \in \mathset{J} \\
        & & & x \in \mathset{X}
    }
    If \progref{prog:fixedyNLP} is infeasible the NLP feasibility subproblem for fixed $y^k$ can be solved.
    \program{prog:feasibleNLP}{
        & \minimize{x} & & u \\
        & \st & & g_j(x,y^k) \leq u, \; \; j \in \mathset{J} \\
        & & & x \in \mathset{X}, \; \; u \in \mathbb{R}
    }
    This NLP returns a strictly positive value for $u$.

    Aside from the presented NLP's a linearized version of \progref{prog:stdMINLP} is regularly solved
    \program{prog:MILP}{
        & \minimize{x, y} & & C_L^k = \alpha \\
        & \st & & f(x^k,y^k) + \nabla f(x^k,y^k)^T \begin{bmatrix} x - x^k \\ y - y^k \end{bmatrix} \leq \alpha \\
        & & & g_j(x^k,y^k) + \nabla g_j(x^k,y^k)^T \begin{bmatrix} x - x^k \\ y - y^k \end{bmatrix} \leq 0, \; \; j \in \mathset{J} \\
        & & & x \in \mathset{X}, \; \; y \in \mathset{Y}, \; \; k = {1 \dots K}
    }
    The linearized problem can be constructed in several ways from the set of points $K$ attained in previous iterations.
    Sometimes only violated or active constraints are linearized. When objective function and constraints are convex, the
    objective function is underestimated, while the constraints are overestimated. From the overestimated constraints stems
    the name outer approximation.

        \subsubsection{Branch \& bound (B\&B)}
        The branch and bound (B\&B) algorithm has originally been proposed for linear problems, but has since been extended to
        handle non-linear objectives and constraints. As an initial step for the branch and bound the fully relaxed
        NLP \progref{prog:relaxedNLP} is solved, which, as mentioned before, provides an absolute lower bound to
        the program in question. In the rare case that all relaxed integer variables assume integer values the optimal
        solution has been found and the algorithm can terminate. Otherwise a tree search is performed, exploring the
        space of the integer variables. In each step an increasing number of integer variables is fixed such that
        a NLP in the form of \progref{prog:fixedyNLP} needs to be solved. The solution of these subproblems is
        a new lower bound for all descendant nodes. Further exploration of branches can be stopped, once a
        given subproblem returns a value grater than the current upper bound or becomes infeasible.

        Due to the large number of NLP subproblems that have to be solved within the tree search, the branch and bound
        algorithm is most attractive, if the solution is computationally inexpensive, or few nodes have to be explored.

        \subsubsection{Outer approximation (OA)}
        The outer approximation (OA) algorithm relies on consecutively solving \progref{prog:fixedyNLP} and
        \progref{prog:MILP}. Each solution of the NLP with fixed $y^k$ yields a new point $(x^k,y^k)$ which is used to
        construct an updated version of the MILP. The MILP generally includes linearized versions of all constraints and
        the objective function. As more and more points become available during the iterative process, new constraints
        are constructed for each available point.

        The main theorem for the derivation of the OA algorithm states, that the optimal solution of the problem
        \progref{prog:MILP} constructed from all points $(x^k,y^k), k \in K^{\ast}$. Where $K^{\ast}$ is made up of
        all optimal solutions of \progref{prog:fixedyNLP} where the current $y^k$ yields a feasible solution, and
        \progref{prog:feasibleNLP} where infeasible solutions of the NLP with fixed $y^k$ are encountered. It should
        again be emphasized, that this theorem holds only for convex a objective function and constraints.

        As the points necessary to construct the aforementioned system are not available when the solution process
        commences, a smaller systems is constructed and extended as more points become available. Initially again
        the fully relaxed problem is solved. This again yields an absolute lower bound for the original problem. The solution of each consecutive
        MILP gives a new lower bound which will always be greater than the bounds from previous iterations. Without any
        prove this argument is supported by the fact that adding new linear constraints will limit the feasible region of
        the problem and hence further restrict the possible solutions for the continuous variables.

        The optimal points attained from the NLP subproblems with fixed discrete values form upper bounds on the optimal
        solution. Here no statement can be made about the quality of the bound in each step, but rather is the current
        upper bound updated, once a lower value is encountered.

        The iterative process terminates, once the current upper and lower bound are within a given tolerance. It can be
        pointed out, that the outer approximation algorithm converges to the optimal solution in a single iteration if
        objective function and constraints in the original problem are linear, since in that case problems
        \progref{prog:stdMINLP} and \progref{prog:MILP} are equivalent.

        \subsubsection{Generalized Benders decomposition (GBD)}
        The generalized Benders decomposition is very similar to the outer approximation algorithm. The main difference
        lies in way that \progref{prog:MILP} is constructed. While for the outer approximation problem all constraints
        are included, in case of the GBD only active constraints are $J^k = \{j \; | \; g_j(x^k,y^k) = 0\}$ considered.
        Furthermore the set of continuous variables is eliminated by considering the KKT-conditions. The resulting problem
        \program{}{
            & \minimize{y} & & C^K_L = \alpha \\
            & \st & & f(x^k,y^k) + \nabla_y f(x^k,y^k)^T (y - y^k) \\
            & & & \; \;  + (\mu^k)^T \left[ g(x^k,y^k) + \nabla_y g(x^k,y^k)^T (y - y^k) \right] \leq \alpha, \; \; k \in \mathcal{K}_{FS} \\
            & & & (\lambda^k)^T \left[ g(x^k,y^k) + \nabla_y g(x^k,y^k)^T (y - y^k) \right] \leq 0, \; \; k \in \mathcal{K}_{IS} .
        }
        Where $\mathcal{K}_{FS}$ and $\mathcal{K}_{IS}$ denote the sets of solutions for of feasible and infeasible
        iterations respectively.

        This difference in formulation the MILP in the OA and GBD algorithm is what leads to the major computational differences.
        In general the MILP in the OA algorithm is expected to return tighter bounds than the respective
        problem in GBD. However the computational effort to solve the problem might be considerably higher for the
        OA case. Hence the GBD is expected to need more iterations, before converging, while the cost of a single
        iteration might be lower.

        \subsubsection{Extended cutting plane (ECP)}
        The extended cutting plane method requires no solution any NLP subproblems. Rather a version of the linearized
        problem is solved during each iteration. After each iteration the problem is extended by an addition cut or
        linearized constraint. Most commonly a linear version of the most violated constraint is added. A further
        possibility is to add all violated constraints to the problem. It should be noted, that for the ECP method
        the objective function has to be linear. If necessary non-linearities in the objective function can be moved to the
        constraints by introduction of new variables.


    \subsection{Continuous reformulation}
    \label{sec:opt:theory:continuous}
    An alternative approach to solving discrete-continuous problems is to reformulate them as NLP
    and introduce further constraints which ensure that the discrete decision variables will assume integer values.
    Several different authors have proposed ways of reformulating integer decisions. One common way is to introduce
    the so called complementary or equilibrium constraints which lead to problems that must be dealt with designated
    (NLP) solvers. Despite the fact that they posses poor theoretical features they have successfully been applied
    to problems of different scales. Lately however, alternative approaches have been suggested, which can be implemented
    in \gproms with relative ease and then solved with the NLP solvers included in the modeling environment.
    The first approach presented formulates a so called non-linear complementary problem (NCP) by introducing
    further constraints. Secondly within the distributed stream method, differentiable distribution functions (DDF)
    are formulated to model the associated integer decisions. In the following both approaches will be summarized briefly.
    A more detailed discussion of continuous reformulation techniques can be found in \cite{Stein.2004}.
    \begin{figure}
        \scriptsize
        \center
        \begin{subfigure}{0.48\textwidth}
            \input{GNUPlot/FB_Plot}
            \caption{plot of Fischer-Burmeister function.}
            \label{fig:FB_plot}
        \end{subfigure}
            \begin{subfigure}{0.48\textwidth}
            \input{GNUPlot/DDF_Plot}
            \caption{plot of DDF function.}
            \label{fig:DDF_plot}
        \end{subfigure}
    \end{figure}
    \todo{axis label for ddf and fb plot}

        \subsubsection{Non-linear complementary problem (NCP)}
        To arrive at a formulation with favourable theoretical properties Kraemer et al. \cite{Kraemer.2009} proposed
        a continuous reformulation based on the Fischer-Burmeister function
        \Eq{eq:opt:FB}{
            1 - \sqrt{y_i^2 + (1 - y_i)^2} = 0.
        }
        The zero set of this function contains exactly the values zero and and hence forces a continuous variable to
        take either integer value. However it is still very hard to solve complex, large-scale problems by simply
        enforcing this constraint. In order to facilitate convergence, the FB-function can be relaxed to a fully
        continuous problem
        \Eq{eq:opt:FB}{
            1 - \sqrt{y_i^2 + (1 - y_i)^2} \leq \mu.
        }
        The relaxation factor $\mu$ is then gradually reduced to zero in a sequence of NLP problems. \Figref{fig:FB_plot}
        shows a plot of the FB-function over the domain zero to one. The green lines on the x-axis denote the feasible domain
        for a given value of $\mu$. As one can see, if the relaxation factor is chosen large enough the entire span
        between zero and one is feasible, while for decreasing values the feasible set becomes disjunct and continuous
        variables are forced to either side to take on integer values.

        \subsubsection{Differentiable distribution function (DDF)}
        An alternative way of reaching a continuous formulation was brought fourth by Lang and Biegler \cite{Lang.2002}
        who within their distributed stream method propose the introduction of a differentiable distribution function (DDF)
        \Eq{}{
            \Psi_j = \frac{ \exp\left[ - \left( \frac{j - \sum_i \zeta_i i}{\sigma} \right)^2 \right] }{
                \sum_k \exp\left[ - \left( \frac{k - \sum_i \zeta_i i}{\sigma} \right)^2 \right]}.
        }
        This function models a normal distribution around a central tray. As the standard deviation ($\sigma$) is reduced,
        the spike at a given tray becomes more significant. Again in a series of NLP's $\sigma$ is driven to small value.
        At $\sigma = 0.25$ one can numerically consider the solution as integer. \Figref{fig:DDF_plot} shows a plot
        of the DDF function for a continuous stage value of 4.5 and 9.3 at $\sigma = 2, 1, 0.5$. Please note, that the
        values for each stage, denoted by dots, must add up to one. Depending on the distribution, a continuous plot can take
        on values larger than one.
